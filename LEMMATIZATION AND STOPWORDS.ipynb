{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definition\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "print(wnl.lemmatize('definitions'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing words by specifying parts-of-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjective:  running\n",
      "Adverb:  running\n",
      "Noun:  running\n",
      "Verb:  run\n"
     ]
    }
   ],
   "source": [
    "print('Adjective: ', wnl.lemmatize('running', pos='a'))\n",
    "print('Adverb: ', wnl.lemmatize('running', pos='r'))\n",
    "print('Noun: ', wnl.lemmatize('running', pos='n'))\n",
    "print('Verb: ', wnl.lemmatize('running', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = ['dictionaries', 'dictionary', \n",
    "                'hushed', 'hush', 'hushing',\n",
    "                'functional', 'functionally',\n",
    "                'lying', 'lied', 'lies',\n",
    "                'flawed', 'flaws', 'flawless', \n",
    "                'friendship', 'friendships', 'friendly', 'friendless', \n",
    "                'definitions', 'definition', 'definitely',  \n",
    "                'the', 'these', 'those',\n",
    "                'motivational', 'motivate', 'motivating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss =  SnowballStemmer('english')\n",
    "\n",
    "ss_stemmed_tokens = []\n",
    "for token in input_tokens:\n",
    "    ss_stemmed_tokens.append(ss.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl_lemmatized_tokens = []\n",
    "for token in input_tokens:\n",
    "    wnl_lemmatized_tokens.append(wnl.lemmatize(token, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>Snowball Stemmer</th>\n",
       "      <th>WordNet Lemmatizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dictionaries</td>\n",
       "      <td>dictionari</td>\n",
       "      <td>dictionaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dictionary</td>\n",
       "      <td>dictionari</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hushed</td>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hushing</td>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>functional</td>\n",
       "      <td>function</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>functionally</td>\n",
       "      <td>function</td>\n",
       "      <td>functionally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lying</td>\n",
       "      <td>lie</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lied</td>\n",
       "      <td>lie</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lies</td>\n",
       "      <td>lie</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>flawed</td>\n",
       "      <td>flaw</td>\n",
       "      <td>flaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>flaws</td>\n",
       "      <td>flaw</td>\n",
       "      <td>flaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>flawless</td>\n",
       "      <td>flawless</td>\n",
       "      <td>flawless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>friendship</td>\n",
       "      <td>friendship</td>\n",
       "      <td>friendship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>friendships</td>\n",
       "      <td>friendship</td>\n",
       "      <td>friendships</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>friendly</td>\n",
       "      <td>friend</td>\n",
       "      <td>friendly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>friendless</td>\n",
       "      <td>friendless</td>\n",
       "      <td>friendless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>definitions</td>\n",
       "      <td>definit</td>\n",
       "      <td>definitions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>definition</td>\n",
       "      <td>definit</td>\n",
       "      <td>definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>definitely</td>\n",
       "      <td>definit</td>\n",
       "      <td>definitely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>these</td>\n",
       "      <td>these</td>\n",
       "      <td>these</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>those</td>\n",
       "      <td>those</td>\n",
       "      <td>those</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>motivational</td>\n",
       "      <td>motiv</td>\n",
       "      <td>motivational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>motivate</td>\n",
       "      <td>motiv</td>\n",
       "      <td>motivate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>motivating</td>\n",
       "      <td>motiv</td>\n",
       "      <td>motivate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           words Snowball Stemmer WordNet Lemmatizer\n",
       "0   dictionaries       dictionari       dictionaries\n",
       "1     dictionary       dictionari         dictionary\n",
       "2         hushed             hush               hush\n",
       "3           hush             hush               hush\n",
       "4        hushing             hush               hush\n",
       "5     functional         function         functional\n",
       "6   functionally         function       functionally\n",
       "7          lying              lie                lie\n",
       "8           lied              lie                lie\n",
       "9           lies              lie                lie\n",
       "10        flawed             flaw               flaw\n",
       "11         flaws             flaw               flaw\n",
       "12      flawless         flawless           flawless\n",
       "13    friendship       friendship         friendship\n",
       "14   friendships       friendship        friendships\n",
       "15      friendly           friend           friendly\n",
       "16    friendless       friendless         friendless\n",
       "17   definitions          definit        definitions\n",
       "18    definition          definit         definition\n",
       "19    definitely          definit         definitely\n",
       "20           the              the                the\n",
       "21         these            these              these\n",
       "22         those            those              those\n",
       "23  motivational            motiv       motivational\n",
       "24      motivate            motiv           motivate\n",
       "25    motivating            motiv           motivate"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems_lemmas_df = pd.DataFrame({\n",
    "    'words': input_tokens,\n",
    "    'Snowball Stemmer': ss_stemmed_tokens,\n",
    "    'WordNet Lemmatizer': wnl_lemmatized_tokens\n",
    "})\n",
    "\n",
    "stems_lemmas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to be the very best,\n",
      "Like no one ever was.\n",
      "To catch them is my real test,\n",
      "To train them is my cause!\n",
      "(I will travel across the land,\n",
      "Searching far and wide.\n",
      "Each Pokemon to understand\n",
      "The power that's inside!)\n",
      "Pokemon!\n",
      "Gotta catch em' all!\n",
      "It's you and me,\n",
      "I know it's my destiny!\n",
      "Pokemon!\n",
      "Oh, you're my best friend,\n",
      "In a world we must defend!\n",
      "Pokemon!\n",
      "Gotta catch em' all!\n",
      "(A heart so true,\n",
      "Our courage will pull us through!)\n",
      "You teach me and I'll teach you,\n",
      "Po-ke-mon!\n",
      "Gotta catch em' all!\n",
      "Gotta catch em' all!\n",
      "Every challenge along the way,\n",
      "With courage I will face!\n",
      "I will battle every day,\n",
      "To claim my rightful place!\n",
      "Come with me, the time is right,\n",
      "There's no better team!\n",
      "Arm in arm, we'll win the fight,\n",
      "It's always been our dream!\n",
      "Pokemon!\n",
      "Gotta catch em' all!\n",
      "It's you and me,\n",
      "I know it's my destiny!\n",
      "Pokemon!\n",
      "Oh, you're my best friend,\n",
      "In a world we must defend!\n",
      "Pokemon!\n",
      "Gotta catch em' all!\n",
      "(A heart so true,\n",
      "Our courage will pull us through!)\n",
      "You teach me and I'll teach you,\n",
      "Po-ke-mon!\n",
      "Gotta catch em' all!\n",
      "Gotta catch em' all!\n",
      "Gotta catch em' all!\n",
      "Gotta catch em' all!\n",
      "Gotta catch em' all!\n",
      "Pokemon!\n",
      "Gotta catch em' all!\n",
      "It's you and me,\n",
      "I know it's my destiny!\n",
      "Pokemon!\n",
      "Oh, you're my best friend,\n",
      "In a world we must defend!\n",
      "Pokemon!\n",
      "Gotta catch em' all!\n",
      "(A heart so true,\n",
      "Our courage will pull us through!)\n",
      "You teach me and I'll teach you,\n",
      "Po-ke-mon!\n",
      "Gotta catch em' all!\n",
      "Gotta catch em' all!\n",
      "Po-ke-mon!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "with open('pokemon.txt', 'r') as f:\n",
    "    file_contents = f.read()\n",
    "    \n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "lemmatized_words = []\n",
    "\n",
    "for word in word_tokens:\n",
    "    lemmatized_words.append(wnl.lemmatize(word, pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I want to be the very best , Like no one ever be . To catch them be my real test , To train them be my cause ! ( I will travel across the land , Searching far and wide . Each Pokemon to understand The power that 's inside ! ) Pokemon ! Got ta catch em ' all ! It 's you and me , I know it 's my destiny ! Pokemon ! Oh , you 're my best friend , In a world we must defend ! Pokemon ! Got ta catch em ' all ! ( A heart so true , Our courage will pull us through ! ) You teach me and I 'll teach you , Po-ke-mon ! Got ta catch em ' all ! Got ta catch em ' all ! Every challenge along the way , With courage I will face ! I will battle every day , To claim my rightful place ! Come with me , the time be right , There 's no better team ! Arm in arm , we 'll win the fight , It 's always be our dream ! Pokemon ! Got ta catch em ' all ! It 's you and me , I know it 's my destiny ! Pokemon ! Oh , you 're my best friend , In a world we must defend ! Pokemon ! Got ta catch em ' all ! ( A heart so true , Our courage will pull us through ! ) You teach me and I 'll teach you , Po-ke-mon ! Got ta catch em ' all ! Got ta catch em ' all ! Got ta catch em ' all ! Got ta catch em ' all ! Got ta catch em ' all ! Pokemon ! Got ta catch em ' all ! It 's you and me , I know it 's my destiny ! Pokemon ! Oh , you 're my best friend , In a world we must defend ! Pokemon ! Got ta catch em ' all ! ( A heart so true , Our courage will pull us through ! ) You teach me and I 'll teach you , Po-ke-mon ! Got ta catch em ' all ! Got ta catch em ' all ! Po-ke-mon !\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A bird in hand is worth two in the bush. Good things come to those who wait. These watches cost $1500!  There are other fish in the sea. The ball is in your court. Mr. Smith Goes to Washington  Doogie Howser M.D.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_array = [\"A bird in hand is worth two in the bush.\",\n",
    "              \"Good things come to those who wait.\",\n",
    "              \"These watches cost $1500! \",\n",
    "              \"There are other fish in the sea.\",\n",
    "              \"The ball is in your court.\",\n",
    "              \"Mr. Smith Goes to Washington \",\n",
    "              \"Doogie Howser M.D.\"]\n",
    "\n",
    "text = \" \".join(text_array)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'bird',\n",
       " 'in',\n",
       " 'hand',\n",
       " 'is',\n",
       " 'worth',\n",
       " 'two',\n",
       " 'in',\n",
       " 'the',\n",
       " 'bush',\n",
       " '.',\n",
       " 'Good',\n",
       " 'things',\n",
       " 'come',\n",
       " 'to',\n",
       " 'those',\n",
       " 'who',\n",
       " 'wait',\n",
       " '.',\n",
       " 'These',\n",
       " 'watches',\n",
       " 'cost',\n",
       " '$',\n",
       " '1500',\n",
       " '!',\n",
       " 'There',\n",
       " 'are',\n",
       " 'other',\n",
       " 'fish',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sea',\n",
       " '.',\n",
       " 'The',\n",
       " 'ball',\n",
       " 'is',\n",
       " 'in',\n",
       " 'your',\n",
       " 'court',\n",
       " '.',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " 'Goes',\n",
       " 'to',\n",
       " 'Washington',\n",
       " 'Doogie',\n",
       " 'Howser',\n",
       " 'M.D',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'bird', 'hand', 'worth', 'two', 'bush', '.', 'Good', 'things', 'come', 'wait', '.', 'These', 'watches', 'cost', '$', '1500', '!', 'There', 'fish', 'sea', '.', 'The', 'ball', 'court', '.', 'Mr.', 'Smith', 'Goes', 'Washington', 'Doogie', 'Howser', 'M.D', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_words = []\n",
    "\n",
    "for word in word_tokens:\n",
    "    if word not in stop_words:\n",
    "        filtered_words.append(word)\n",
    "        \n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRYING ON DEEP LEARNING ARTICLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "para=\"\"\"One of the most common AI techniques used for processing big data is machine learning, a self-adaptive algorithm that gets increasingly better analysis and patterns with experience or with newly added data.\n",
    "\n",
    "If a digital payments company wanted to detect the occurrence or potential for fraud in its system, it could employ machine learning tools for this purpose. The computational algorithm built into a computer model will process all transactions happening on the digital platform, find patterns in the data set, and point out any anomaly detected by the pattern.\n",
    "\n",
    "Deep learning, a subset of machine learning, utilizes a hierarchical level of artificial neural networks to carry out the process of machine learning. The artificial neural networks are built like the human brain, with neuron nodes connected together like a web. While traditional programs build analysis with data in a linear way, the hierarchical function of deep learning systems enables machines to process data with a nonlinear approach.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'common', 'AI', 'techniques', 'used', 'processing', 'big', 'data', 'machine', 'learning', ',', 'self-adaptive', 'algorithm', 'gets', 'increasingly', 'better', 'analysis', 'patterns', 'experience', 'newly', 'added', 'data', '.', 'If', 'digital', 'payments', 'company', 'wanted', 'detect', 'occurrence', 'potential', 'fraud', 'system', ',', 'could', 'employ', 'machine', 'learning', 'tools', 'purpose', '.', 'The', 'computational', 'algorithm', 'built', 'computer', 'model', 'process', 'transactions', 'happening', 'digital', 'platform', ',', 'find', 'patterns', 'data', 'set', ',', 'point', 'anomaly', 'detected', 'pattern', '.', 'Deep', 'learning', ',', 'subset', 'machine', 'learning', ',', 'utilizes', 'hierarchical', 'level', 'artificial', 'neural', 'networks', 'carry', 'process', 'machine', 'learning', '.', 'The', 'artificial', 'neural', 'networks', 'built', 'like', 'human', 'brain', ',', 'neuron', 'nodes', 'connected', 'together', 'like', 'web', '.', 'While', 'traditional', 'programs', 'build', 'analysis', 'data', 'linear', 'way', ',', 'hierarchical', 'function', 'deep', 'learning', 'systems', 'enables', 'machines', 'process', 'data', 'nonlinear', 'approach', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(para)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_words = []\n",
    "\n",
    "for word in word_tokens:\n",
    "    if word not in stop_words:\n",
    "        filtered_words.append(word)\n",
    "        \n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
